{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from CircuitSimulation.CircuitSimulator import *\n",
    "import numpy as np\n",
    "from diffusers import UNet2DModel\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Circuit shape, with input coloumn left, and output column right\n",
    "# [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "# [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "# [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "# [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "# [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "# [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "# [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "# [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "# [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "# [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "\n",
    "InitialDataset = torch.tensor([\n",
    "    [\n",
    "        [1, 1, 1, 1, 1, 2, 1, 1, 1, 1],\n",
    "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ],\n",
    "    [\n",
    "        [1, 1, 1, 1, 0, 0, 0, 0, 1, 1],\n",
    "        [0, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
    "        [1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
    "        [0, 1, 0, 1, 0, 0, 1, 1, 0, 0],\n",
    "        [0, 1, 1, 2, 1, 1, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    ]\n",
    "])\n",
    "\n",
    "InitialLabels = torch.tensor([\n",
    "    [\n",
    "        [1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2 ],\n",
    "        [1, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2 ],\n",
    "        [0, 1, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2 ],\n",
    "        [0, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2 ],\n",
    "    ],\n",
    "    [\n",
    "        [1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2 ],\n",
    "        [1, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2 ],\n",
    "        [0, 1, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2 ],\n",
    "        [0, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2 ],\n",
    "    ],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{inp0: {1}, inp1: {3}, out0: {2}, AND0base: {3}, AND0collector: {1}, AND0emitter: {2}}\n",
      "{1: [inp0, AND0collector], 3: [inp1, AND0base], 2: [out0, AND0emitter]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(inp0, True), (inp1, True), (out0, True)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test that circuits work\n",
    "\n",
    "testCircuit = InitialDataset[1].numpy()\n",
    "\n",
    "socket1 = Socket(\"inp0\", True)\n",
    "socket2 = Socket(\"inp1\", True)\n",
    "socket3 = Socket(\"out0\", False)\n",
    "\n",
    "socketList = [\n",
    "    (socket1, (0, 0)), \n",
    "    (socket2, (0, 3)), \n",
    "    (socket3, (9, 0)),  \n",
    "    ]\n",
    "\n",
    "socketMap = GetSocketMap(testCircuit, socketList)\n",
    "connectionMap = GetConnectionMap(socketMap)\n",
    "\n",
    "print(socketMap)\n",
    "print(connectionMap)\n",
    "testOrder1 = [socket1, socket2]\n",
    "Simulate(connectionMap, socketMap, testOrder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4526e+00,  4.6408e-01, -2.2007e-02, -5.0593e-02, -1.8623e+00,\n",
       "          9.8007e-01,  8.1641e-02,  3.1947e-02,  1.0452e-01,  7.8182e-01,\n",
       "          1.2370e+00, -1.2544e+00, -1.0294e+00, -4.0757e-01, -1.6950e+00,\n",
       "          1.1876e+00],\n",
       "        [ 8.4392e-01, -2.5650e-01, -1.5884e-01,  2.4626e-01, -2.4257e+00,\n",
       "          1.2766e+00,  5.2654e-01,  7.1567e-01,  1.9325e-01,  7.9750e-01,\n",
       "          8.2815e-01, -7.9608e-01, -1.0337e+00, -8.1231e-01, -1.2194e+00,\n",
       "          1.2746e+00],\n",
       "        [ 9.5773e-01, -1.5920e-01, -4.0058e-02,  9.0168e-02, -1.6174e+00,\n",
       "          1.5712e+00,  8.1230e-01,  1.2431e+00, -2.3086e-01,  3.5523e-01,\n",
       "          1.1842e-01, -2.2396e-01, -1.0856e+00, -1.3408e+00, -1.7145e+00,\n",
       "          1.2643e+00],\n",
       "        [ 1.0282e+00,  1.8311e-01, -1.9624e-03, -1.6950e-01, -2.0915e+00,\n",
       "          9.5756e-01,  4.9463e-01,  1.1232e+00,  1.7706e-01,  2.4930e-01,\n",
       "          1.0046e+00, -1.3411e+00, -8.0515e-01, -8.6900e-01, -1.3438e+00,\n",
       "          1.4043e+00]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RowEmbedder(nn.Module):\n",
    "    def __init__(self, num_categories, vector_length, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.shared_embed = nn.Embedding(num_categories, embedding_dim)\n",
    "        self.position_weights = nn.Parameter(torch.ones(vector_length, embedding_dim))\n",
    "        self.position_bias = nn.Parameter(torch.zeros(vector_length, embedding_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, vector_length]\n",
    "        shared = self.shared_embed(x)  # [batch_size, vector_length, emb_dim]\n",
    "        # Apply position-specific scaling and shifting\n",
    "        return shared * self.position_weights + self.position_bias\n",
    "    \n",
    "\n",
    "class TabularTransformer(nn.Module):\n",
    "    def __init__(self, num_categories, num_features, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.row_embedding = RowEmbedder(num_categories, num_features, d_model) #num_categories, vector_length, embedding_dim\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=6,\n",
    "            dim_feedforward=4*d_model,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=6,\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #batch_size, num_rows, num_cols = x.shape\n",
    "\n",
    "        rows = self.row_embedding(x) #bs, rows, columns, embedding\n",
    "        rows = rows.mean(dim=2)\n",
    "    \n",
    "        transformed = self.transformer(rows)\n",
    "\n",
    "        return transformed\n",
    "\n",
    "transformer = TabularTransformer(3, 12, 16)\n",
    "\n",
    "transformer(InitialLabels)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# betas = torch.linspace(0.0001, 0.014, 200)\n",
    "# alphas = 1.0 - betas\n",
    "# alphaCumprod = torch.cumprod(alphas, dim=0) \n",
    "# alphaCumprod[-1] # this should be roughly 1/4\n",
    "\n",
    "class CategoricalScheduler:\n",
    "    def __init__(self, TrainSteps = 200, numCategories = 4, betaStart = 0.0001, betaEnd = 0.014):\n",
    "        self.TrainSteps = TrainSteps\n",
    "        self.noiseDevice = 'cpu'\n",
    "        self.numCategories = numCategories\n",
    "\n",
    "        self.betas = torch.linspace(betaStart, betaEnd, TrainSteps, device=self.noiseDevice)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "         # The last value of alpha_cumprod should be close to 1/numclasses\n",
    "\n",
    "    def addNoise(self, imageBatch, time):\n",
    "        # imagebatch shape is (128, 11, 32, 32), where the 11 is one-hot vector of categories\n",
    "        bs, ch, w, h = imageBatch.shape\n",
    "\n",
    "        with torch.no_grad():\n",
    "            alpha_t = self.alpha_cumprod[time].view(-1, 1, 1, 1) # Translates shape (1,) -> (1, 1, 1, 1)\n",
    "\n",
    "            # the per pixel probability distribution of the categories\n",
    "            currentProbabilities = imageBatch\n",
    "\n",
    "            # The chance of each state per pixel when noised            \n",
    "            updatedProbabilities = currentProbabilities * alpha_t + (1 - alpha_t) / self.numCategories \n",
    "            updatedProbabilities = updatedProbabilities.permute(0, 2, 3, 1) # reshape such that it is flattened correctly below\n",
    "            updatedProbabilities = updatedProbabilities.reshape(bs*w*h, self.numCategories)  # Shape: [bs * w * h, 11]\n",
    "            \n",
    "\n",
    "            # 1 Sample per value\n",
    "            categoricalNoise = torch.multinomial(updatedProbabilities, 1, replacement=True)\n",
    "            categoricalNoise = categoricalNoise.view(bs, w, h) # Shape: [bs, w, h]\n",
    "\n",
    "            noisedImages = F.one_hot(categoricalNoise, num_classes=self.numCategories)\n",
    "            noisedImages = noisedImages.permute(0, 3, 1, 2) # [bs, num_classes, w, h]\n",
    "\n",
    "            return noisedImages\n",
    "\n",
    "scheduler = CategoricalScheduler()\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "class CategoricalDiffusionModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.guidance_prob=0.1\n",
    "\n",
    "    embeddingSize = ... # how much?\n",
    "\n",
    "    self.model = UNet2DModel(\n",
    "        sample_size=28,           # the target image resolution\n",
    "        in_channels=1 + embeddingSize, # Additional input channels for class cond.\n",
    "        out_channels=1 + embeddingSize,           # the number of output channels\n",
    "        layers_per_block=2,       # how many ResNet layers to use per UNet block\n",
    "        block_out_channels=(64, 64, 64), \n",
    "        down_block_types=( \n",
    "            \"DownBlock2D\",        # a regular ResNet downsampling block\n",
    "            \"DownBlock2D\",    # a ResNet downsampling block with spatial self-attention\n",
    "            \"AttnDownBlock2D\",\n",
    "        ), \n",
    "        up_block_types=(\n",
    "            \"AttnUpBlock2D\", \n",
    "            \"UpBlock2D\",      # a ResNet upsampling block with spatial self-attention\n",
    "            \"UpBlock2D\",          # a regular ResNet upsampling block\n",
    "          ),\n",
    "    )\n",
    "  # Our forward method now takes the class labels as an additional argument\n",
    "  def forward(self, x, t, class_labels):\n",
    "    # Shape of x:\n",
    "    bs, ch, w, h = x.shape\n",
    "    \n",
    "    # class conditioning in right shape to add as additional input channels\n",
    "    class_cond = self.class_emb(class_labels) # Map to embedding dimension\n",
    "    batch_size, embed_dim = class_cond.shape\n",
    "    mask = torch.rand(batch_size, device=class_cond.device) < self.guidance_prob\n",
    "    class_cond[mask] = torch.zeros(embed_dim, device=class_cond.device)\n",
    "    class_cond = class_cond.view(bs, class_cond.shape[1], 1, 1).expand(bs, class_cond.shape[1], w, h)\n",
    "    # x is shape (bs, 1, 28, 28) and class_cond is now (bs, 4, 28, 28)\n",
    "    \n",
    "    # Net input is now x and class cond concatenated together along dimension 1\n",
    "    net_input = torch.cat((x, class_cond), 1) # (bs, 5, 28, 28)\n",
    "\n",
    "    # Feed this to the UNet alongside the timestep and return the prediction\n",
    "    return self.model(net_input, t).sample # (bs, 11, 28, 28)\n",
    "  \n",
    "model = CategoricalDiffusionModel().to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
