{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How should I use truth table as embedding, where the output can be in witin the input?\n",
    "\n",
    "\n",
    "A single row may look like:\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Latch example</th>\n",
    "    <th colspan=\"4\">Input</th>\n",
    "    <th colspan=\"2\">Output</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Value</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Variable name</td>\n",
    "    <td>inp0</td>\n",
    "    <td>inp1</td>\n",
    "    <td>out0</td>\n",
    "    <td>out1</td>\n",
    "    <td>out0</td>\n",
    "    <td>out1</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "For variable name, use categorical labels of size 2. It should be fine to just concatenate the values and names as they would both be binary.\n",
    "\n",
    "Ahh, but it also needs a n/a for empty values\n",
    "\n",
    "Maybe have the vector be (0, 1, N/A), and then the types are fixed by position. Make input have 16 values, and output have 8 values. That is up to 8 inputs and 8 outputs.\n",
    "\n",
    "\n",
    "For embedding the entire table. Take all the row embeddings and use a transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1049,  1.5537,  0.3037,  0.0900,  0.8772],\n",
       "         [ 0.1049,  1.5537,  0.3037,  0.0900,  0.8772],\n",
       "         [ 0.1049,  1.5537,  0.3037,  0.0900,  0.8772]],\n",
       "\n",
       "        [[-0.6277, -0.7029,  0.8341,  0.5222,  1.4537],\n",
       "         [ 0.1049,  1.5537,  0.3037,  0.0900,  0.8772],\n",
       "         [-0.6277, -0.7029,  0.8341,  0.5222,  1.4537]],\n",
       "\n",
       "        [[-0.6277, -0.7029,  0.8341,  0.5222,  1.4537],\n",
       "         [-0.6277, -0.7029,  0.8341,  0.5222,  1.4537],\n",
       "         [-0.6277, -0.7029,  0.8341,  0.5222,  1.4537]],\n",
       "\n",
       "        [[ 0.1049,  1.5537,  0.3037,  0.0900,  0.8772],\n",
       "         [-0.6277, -0.7029,  0.8341,  0.5222,  1.4537],\n",
       "         [-0.6277, -0.7029,  0.8341,  0.5222,  1.4537]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "\n",
    "#What does nn.parameter do?\n",
    "\n",
    "# I really do not like working with enums in Python. Making a truthtable where every element is\n",
    "# SocketState.state.value is incredibly verbose, and why is there not a better way to infer using \n",
    "# the integer values!\n",
    "# class SocketState(Enum):\n",
    "#     OFF = 0\n",
    "#     ON = 1\n",
    "#     UNUSED = 2\n",
    "OFF = 0\n",
    "ON = 1\n",
    "UNUSED = 2\n",
    "\n",
    "\n",
    "class RowEmbedder(nn.Module):\n",
    "    def __init__(self, num_categories, vector_length, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.shared_embed = nn.Embedding(num_categories, embedding_dim)\n",
    "        self.position_weights = nn.Parameter(torch.ones(vector_length, embedding_dim))\n",
    "        self.position_bias = nn.Parameter(torch.zeros(vector_length, embedding_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, vector_length]\n",
    "        shared = self.shared_embed(x)  # [batch_size, vector_length, emb_dim]\n",
    "        # Apply position-specific scaling and shifting\n",
    "        return shared * self.position_weights + self.position_bias\n",
    "\n",
    "\n",
    "# (inp1, inp2) -> (out1)\n",
    "ANDTable = torch.tensor([\n",
    "    [ON, ON, ON],\n",
    "    [OFF, ON, OFF],\n",
    "    [OFF, OFF, OFF],\n",
    "    [ON, OFF, OFF],  \n",
    "])\n",
    "# When i have 2 inputs, there are 2^2 = 4 possible combinations\n",
    "# When i have 16 + 8 -> 24 coloumns, of which 16 are inputs. I would have 2^16 combinations. that would be 65536 rows per table...\n",
    "\n",
    "# But often, some if not most of these would be UNUSED. \n",
    "\n",
    "testEmbedder = RowEmbedder(3, 3, 5)\n",
    "\n",
    "rowsEmbeddded = testEmbedder(ANDTable) \n",
    "print(rowsEmbeddded.shape) \n",
    "rowsEmbeddded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 16])\n",
      "torch.Size([1, 4, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1436, -0.6812, -0.0235,  0.6048, -0.0848,  0.0645, -1.0959,\n",
       "          -1.2769,  0.6558,  0.8663,  1.8017, -0.6553, -2.0930, -0.1557,\n",
       "           0.5661,  1.6507],\n",
       "         [-0.0702, -0.6543,  0.0652,  0.2972, -0.6605,  0.5285, -1.1609,\n",
       "          -1.2212,  0.3690,  0.6083,  1.8073, -0.4109, -1.6503, -0.7006,\n",
       "           0.8382,  2.0152],\n",
       "         [-0.0956, -0.5989,  0.2707, -0.3734, -0.2228,  0.3948, -1.2477,\n",
       "          -1.2040,  0.3847,  1.0032,  2.0413, -0.2551, -1.8056, -0.5681,\n",
       "           0.5696,  1.7069],\n",
       "         [ 0.3364, -0.6442,  0.1417,  0.6431, -0.6152,  0.3515, -1.3678,\n",
       "          -1.4023,  0.4750,  0.9910,  1.7164, -0.3414, -1.8087, -0.5382,\n",
       "           0.4230,  1.6396]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tabluar encoder\n",
    "\n",
    "class TabularTransformer(nn.Module):\n",
    "    def __init__(self, num_features, num_categories, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.row_embedding = RowEmbedder(num_categories, num_features, d_model) #num_categories, vector_length, embedding_dim\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=8,\n",
    "            dim_feedforward=4*d_model,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=6,\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, num_rows, num_cols = x.shape\n",
    "\n",
    "        rows = self.row_embedding(x) #bs, rows, columns, embedding\n",
    "        rows = rows.mean(dim=2)\n",
    "        \n",
    "        print(rows.shape)\n",
    "        transformed = self.transformer(rows)\n",
    "        print(transformed.shape)\n",
    "\n",
    "        return transformed\n",
    "\n",
    "tester = TabularTransformer(3, 3, 16)\n",
    "tester(ANDTable.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 16])\n",
      "torch.Size([1, 4, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0100,  0.9726,  0.3281, -0.7896,  0.8628, -0.7175, -2.1203,\n",
       "           0.0937,  1.6237, -0.6103,  0.5341,  0.6800,  0.9845, -0.9447,\n",
       "          -1.1796, -0.7275],\n",
       "         [ 1.1550,  1.4748,  0.2274, -0.9185,  1.1520, -0.8207, -1.7552,\n",
       "           0.4032,  1.3018, -0.8060,  0.6085,  0.4700,  0.5520, -1.0734,\n",
       "          -1.0766, -0.8944],\n",
       "         [ 1.2859,  0.9271,  0.5452, -0.7221,  0.8489, -0.7952, -2.1205,\n",
       "           0.4179,  1.6112, -0.5901,  0.4462,  0.4519,  0.6022, -1.2450,\n",
       "          -0.8267, -0.8369],\n",
       "         [ 0.9696,  1.3358,  0.6448, -0.8679,  0.8405, -0.1166, -1.6423,\n",
       "           0.0502,  1.2079, -0.9004,  0.6790,  0.6995,  0.8512, -1.2895,\n",
       "          -1.3870, -1.0750]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANDFullTable = torch.tensor([\n",
    "    [ON, ON, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, ON, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED],\n",
    "    [OFF, ON, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, OFF, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED],\n",
    "    [OFF, OFF, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, OFF, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED],\n",
    "    [ON, OFF, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, OFF, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED, UNUSED],  \n",
    "])\n",
    "\n",
    "tableTransformer = TabularTransformer(24, 3, 16)\n",
    "tableTransformer(ANDFullTable.unsqueeze(0))#.mean(dim=1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
